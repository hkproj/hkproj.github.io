---
title: Videos
date: 2024-09-20
---
ğŸ”¥ [Flash Attention derived and coded from first principles with Triton (Python)](https://youtu.be/zy8ChVd_oTM)

ğŸŒ… [Coding a Multimodal (Vision) Language Model from scratch in PyTorch with full explanation](https://www.youtube.com/watch?v=vAmKB7iPkWw)

ğŸª¢ [ML Interpretability: feature visualization, adversarial example, interp. for language models](https://youtu.be/lg1-M8hEX50)

ğŸ•¸ï¸ [Kolmogorov-Arnold Networks: MLP vs KAN, Math, B-Splines, Universal Approximation Theorem](https://youtu.be/-PFIkkwWdnM)

ğŸ¯ [Direct Preference Optimization (DPO) explained: Bradley-Terry model, log probabilities, math derivations](https://youtu.be/hvGa5Mba4c8)

ğŸ“ [Reinforcement Learning from Human Feedback explained with math derivations and the PyTorch code](https://www.youtube.com/watch?v=qGyFrqc34yc)

ğŸ [Mamba and S4 Explained: Architecture, Parallel Scan, Kernel Fusion, Recurrent, Convolution, Math](https://www.youtube.com/watch?v=8Q_tqwpTpVU)

ğŸŒˆ [Mistral 7B and Mixtral 8x7B Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer (KV) Cache, Model Sharding](https://www.youtube.com/watch?v=UiX8K-xBUpE)

ğŸ”¬ [Distributed Training with PyTorch: complete tutorial with cloud infrastructure and code](https://www.youtube.com/watch?v=toUSzwR0EV8)

âš›ï¸ [Quantization explained with PyTorch - Post-Training Quantization, Quantization-Aware Training](https://www.youtube.com/watch?v=0VdNflU08yA)

ğŸ—ƒï¸ [Retrieval Augmented Generation (RAG) Explained: Embedding, Sentence BERT, Vector Database (HNSW)](https://www.youtube.com/watch?v=rhZgXNdhWDY)

ğŸ‘¨ [BERT explained: Training, Inference,  BERT vs GPT/LLamA, Fine tuning, [CLS] token](https://www.youtube.com/watch?v=90mGPxR2GgY)

ğŸŒ„ [Coding Stable Diffusion From Scratch](https://www.youtube.com/watch?v=ZBKpAp_6TGI)

ğŸ¦™ [Coding LLaMA 2 From Scratch](https://www.youtube.com/watch?v=oM4VmoabDAI)

ğŸ¦™ [LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU](https://www.youtube.com/watch?v=Mn_9W1nCFLo)

ğŸŒ [Segment Anything - Model explanation with code](https://www.youtube.com/watch?v=eYhvJR4zFUM)

ğŸ§® [LoRA: Low-Rank Adaptation of Large Language Models - Explained visually + PyTorch code from scratch](https://www.youtube.com/watch?v=PXWYUTMt-AU)

â›“ [LongNet: Scaling Transformers to 1,000,000,000 tokens: Python Code + Explanation](https://www.youtube.com/watch?v=nC2nU9j9DVQ)

ğŸ–¼ [How diffusion models work - explanation and code!](https://www.youtube.com/watch?v=I1sPXkm2NH4)

âš™ï¸ [Variational Autoencoder - Model, ELBO, loss function and maths explained easily!](https://www.youtube.com/watch?v=iwEzwTTalbg)

ğŸ› [Coding a Transformer from scratch on PyTorch, with full explanation, training and inference.](https://www.youtube.com/watch?v=ISNdQcPhsts)

ğŸª¬ [Attention is all you need (Transformer) - Model explanation (including math), Inference and Training](https://www.youtube.com/watch?v=bCz4OMemCcA)
